{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "print(tf.VERSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "xavier = tf.contrib.layers.xavier_initializer()\n",
    "zeros = tf.zeros_initializer()\n",
    "dim_hidden = 1024 #Dim of hidden layer\n",
    "vocabulary_size = 0 #TODO : FIT VOCABULARY SIZE\n",
    "embedding_size = 512 #Size of the word embeddings\n",
    "annotation_L = 196 #Annotation vectors are of the form L*D\n",
    "annotation_D = 512 \n",
    "max_timesteps = 16\n",
    "lamda = 1.0\n",
    "dropout1 = True\n",
    "dropout1_rate = 0.5\n",
    "dropout2 = True\n",
    "dropout2_rate = 0.5\n",
    "consider_z = True\n",
    "consider_y = True\n",
    "batch_size=196"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initial_c0_h0(annotations):\n",
    "    with tf.variable_scope('initial_c0_h0'):\n",
    "        feature_mean = tf.reduce_mean(annotations,axis=1)\n",
    "        c0 = tf.layers.dense(inputs=feature_mean,units=dim_hidden,activation=tf.nn.tanh,kernel_initializer=xavier)\n",
    "        h0 = tf.layers.dense(inputs=feature_mean,units=dim_hidden,activation=tf.nn.tanh,kernel_initializer=xavier)\n",
    "        return c0,h0 #N * dim_hidden both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#UNDER CONSTRUCTION\n",
    "def soft_attention(annotations,h_prev,i):\n",
    "    with tf.variable_scope('soft_attention',reuse=(i != 0)):\n",
    "        #HANDLE WITH CARE\n",
    "        h_extend = tf.expand_dims(e,axis=1) # N * 1 * h\n",
    "        onemat = tf.ones(shape=[batch_size,annotation_L,1]) # N * L * 1\n",
    "        h_large = tf.matmul(onemat,h_extend) # N * L * h\n",
    "        joined = tf.reshape(tf.concat([annotations,h_large],axis=2),[-1,dim_hidden + annotation_D]) # N  L * D + h\n",
    "        e = tf.reshape(tf.layers.dense(inputs=joined,units=1,activation=tf.nn.relu,kernel_initializer=xavier),[-1,annotation_L])\n",
    "        #HANDLE WITH CARE ENDS\n",
    "        alpha = tf.nn.softmax(e) #dimension N * L\n",
    "        beta = tf.layers.dense(inputs=h_prev,units=1,activation=tf.nn.sigmoid) #N * 1\n",
    "        alpha_exp = tf.expand_dims(alpha,axis=1) #N * 1 * L\n",
    "        pre_gating = tf.reshape(tf.matmul(alpha,annotations),[-1,annotation_D]) # (N,1,L) * (N,L,D) = (N,1,D) reshaped to (N,D)\n",
    "        z = tf.multiply(beta,pre_gating) #N * 1 and N * D pointwise multiplication \n",
    "        return z,alpha,beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_annotation(img_features):\n",
    "    with tf.variable_scope('get_annotation_vecs'):\n",
    "        W = tf.get_variable('W',[annotation_D,annotation_D])\n",
    "        features_flat = tf.reshape(img_features,[-1,annotation_D])\n",
    "        features_proj = tf.matmul(features_flat,W)\n",
    "        annotations = tf.reshape(features_proj,[-1,annotation_L,annotation_D])\n",
    "        return annotations # N * L * D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_embedding(one_hot,i):\n",
    "    with tf.variable_scope('word_embedding',reuse=(i != 0)):\n",
    "        word_embedding = tf.get_variable('word_embedding',[vocabulary_size,embedding_size],initializer=tf.random_uniform_initializer(minval=-1.0,maxval=1.0))\n",
    "        return tf.nn.embedding_lookup(word_embedding,one_hot)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_logits(ey,h,z,training,i):\n",
    "    with tf.variable_scope('get_logits',reuse=(i != 0)):\n",
    "        logits = h\n",
    "        if dropout1:\n",
    "            logits = tf.layers.dropout(inputs=logits,rate=dropout1_rate,training=training)\n",
    "        logits = tf.layers.dense(inputs=logits,units=dim_embed,activation=None,kernel_initializer=xavier)\n",
    "        if consider_y:\n",
    "            logits += ey\n",
    "        if consider_z:\n",
    "            logits += tf.layers.dense(inputs=logits,units=dim_embed,activation=None,use_bias=False,kernel_initializer=xavier)\n",
    "        logits = tf.nn.tanh(logits)\n",
    "        if dropout2:\n",
    "            logits = tf.layers.dropout(inputs=logits,rate=dropout2_rate,training=training)\n",
    "        return tf.layers.dense(inputs=logits,units=dim_vocabulary,activation=None,kernel_initializer=xavier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
